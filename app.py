import streamlit as st
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from PIL import Image
import io
import requests
from sklearn.metrics.pairwise import cosine_similarity
import plotly.express as px
import plotly.graph_objects as go
import h5py
import re
from underthesea import word_tokenize
from gensim import corpora, models, similarities
import pickle
import requests
import re
from io import StringIO

# Thi·∫øt l·∫≠p c·∫•u h√¨nh trang
st.set_page_config(
    page_title="H·ªá th·ªëng ƒê·ªÅ xu·∫•t S·∫£n ph·∫©m Shopee",
    page_icon="shopee.png",
    layout="wide",
    initial_sidebar_state="expanded"
)

@st.cache_resource
def load_stop_words():
    STOP_WORD_FILE = 'vietnamese-stopwords.txt'
    with open(STOP_WORD_FILE, 'r', encoding='utf-8') as file:
        stop_words = file.read()
    stop_words = stop_words.split('\n')
    return stop_words

@st.cache_resource
def load_dictionary(path='dictionary.dict'):
    return corpora.Dictionary.load(path)

@st.cache_resource
def load_tfidf_model(path='tfidf_model.tfidf'):
    return models.TfidfModel.load(path)

@st.cache_resource
def load_similarity_index(_tfidf_corpus, num_features):
    return similarities.SparseMatrixSimilarity(_tfidf_corpus, num_features=num_features)

@st.cache_resource
def load_content_gem(path='content_gem.pkl'):
    with open(path, 'rb') as f:
        return pickle.load(f)
    
@st.cache_resource
def load_baseline_model(path='baseline_model.pkl'):
    with open(path, 'rb') as f:
        return pickle.load(f)

@st.cache_data
def load_large_csv_from_gdrive():
    """
    T·∫£i file CSV l·ªõn t·ª´ Google Drive
    """
    # URL ban ƒë·∫ßu 
    url = f"https://drive.google.com/uc?export=download&id={file_id}"
    
    # L·∫•y session cookies
    session = requests.Session()
    response = session.get(url, stream=True)
    
    # Ki·ªÉm tra xem c√≥ c·∫ßn x√°c nh·∫≠n kh√¥ng
    if "confirm" in response.text:
        confirm_match = re.search(r'confirm=([0-9A-Za-z]+)', response.text)
        if confirm_match:
            confirm_token = confirm_match.group(1)
            url = f"{url}&confirm={confirm_token}"
            response = session.get(url, stream=True)
    
    # ƒê·ªçc n·ªôi dung
    content = response.content.decode('utf-8')
    
    # Chuy·ªÉn ƒë·ªïi sang DataFrame
    df = pd.read_csv(StringIO(content))
    
    return df

@st.cache_resource
def load_pkl_from_gdrive(file_id):
    """
    T·∫£i file pickle t·ª´ Google Drive v√† kh√¥i ph·ª•c ƒë·ªëi t∆∞·ª£ng Python
    
    Tham s·ªë:
        file_id (str): ID c·ªßa file tr√™n Google Drive
    
    Tr·∫£ v·ªÅ:
        object: ƒê·ªëi t∆∞·ª£ng Python ƒë∆∞·ª£c kh√¥i ph·ª•c t·ª´ file pickle
    """
    # URL d·∫°ng chia s·∫ª c√¥ng khai c·ªßa Google Drive
    url = f"https://drive.google.com/uc?export=download&id={file_id}"
    
    try:
        # T·∫£i n·ªôi dung file
        response = requests.get(url)
        response.raise_for_status()  # Ki·ªÉm tra l·ªói HTTP
        
        # S·ª≠ d·ª•ng BytesIO ƒë·ªÉ ƒë·ªçc d·ªØ li·ªáu nh·ªã ph√¢n
        data_bytes = BytesIO(response.content)
        
        # Kh√¥i ph·ª•c ƒë·ªëi t∆∞·ª£ng t·ª´ file pickle
        obj = pickle.load(data_bytes)
        
        return obj
    
    except requests.exceptions.RequestException as e:
        st.error(f"L·ªói khi t·∫£i file: {str(e)}")
        return None
    except pickle.UnpicklingError as e:
        st.error(f"L·ªói khi gi·∫£i n√©n file pickle: {str(e)}")
        return None
        

@st.cache_data
def load_data():
    """
    Load v√† cache d·ªØ li·ªáu s·∫£n ph·∫©m t·ª´ file CSV
    Returns:
        DataFrame: DataFrame ch·ª©a d·ªØ li·ªáu s·∫£n ph·∫©m
    """
    try:
        df = pd.read_csv('Products_ThoiTrangNam_1.csv')
        return df
    except Exception as e:
        st.error(f"L·ªói khi ƒë·ªçc file CSV: {e}")
        return pd.DataFrame()  # Tr·∫£ v·ªÅ DataFrame tr·ªëng n·∫øu c√≥ l·ªói
    
@st.cache_data
def load_data_user():
    """
    Load v√† cache d·ªØ li·ªáu s·∫£n ph·∫©m t·ª´ file CSV
    Returns:
        DataFrame: DataFrame ch·ª©a d·ªØ li·ªáu s·∫£n ph·∫©m
    """
    try:
        df = pd.read_csv('Products_ThoiTrangNam_rating.csv')
        return df
    except Exception as e:
        st.error(f"L·ªói khi ƒë·ªçc file CSV: {e}")
        return pd.DataFrame()  # Tr·∫£ v·ªÅ DataFrame tr·ªëng n·∫øu c√≥ l·ªói
    
# T·∫£i c√°c t√†i nguy√™n s·ª≠ d·ª•ng h√†m ƒë√£ ƒë∆∞·ª£c cache
stop_words = load_stop_words()
dictionary = load_dictionary()
tfidf = load_tfidf_model()
content_gem = load_content_gem()
feature_cnt = len(dictionary.token2id)
corpus = [dictionary.doc2bow(text) for text in content_gem]
tfidf_corpus = tfidf[corpus]
index = load_similarity_index(tfidf_corpus, feature_cnt)
products_df = load_data()
baseline_model = load_baseline_model()
user_df = load_large_csv_from_gdrive("1O-iWYMeHA2Epk5L3zg4UHYMJyqrjiI-pOkgIxnrj9dQ")

# 1. S·ª≠ d·ª•ng st.cache_data.clear() ƒë·ªÉ x√≥a cache t·ª´ d·ªØ li·ªáu ƒë√£ l∆∞u trong ƒë√™corator @st.cache_data
import streamlit as st

# if st.button("X√≥a cache d·ªØ li·ªáu"):
#     st.cache_data.clear()
#     st.success("ƒê√£ x√≥a cache d·ªØ li·ªáu th√†nh c√¥ng!")

# # 2. S·ª≠ d·ª•ng st.cache_resource.clear() ƒë·ªÉ x√≥a cache t·ª´ c√°c t√†i nguy√™n ƒë√£ l∆∞u v·ªõi @st.cache_resource
# if st.button("X√≥a cache t√†i nguy√™n"):
#     st.cache_resource.clear()
#     st.success("ƒê√£ x√≥a cache t√†i nguy√™n th√†nh c√¥ng!")

# # 3. X√≥a t·∫•t c·∫£ c√°c lo·∫°i cache
# if st.button("X√≥a t·∫•t c·∫£ cache"):
#     st.cache_data.clear()
#     st.cache_resource.clear()
#     st.success("ƒê√£ x√≥a t·∫•t c·∫£ cache th√†nh c√¥ng!")
if 'option_products' not in st.session_state:
    # Gi·∫£ s·ª≠ b·∫°n c√≥ DataFrame products_df v√† mu·ªën l·∫•y 10 s·∫£n ph·∫©m ng·∫´u nhi√™n
    # C√≥ th·ªÉ thay ƒë·ªïi logic ch·ªçn ng·∫´u nhi√™n t√πy thu·ªôc v√†o y√™u c·∫ßu c·ª• th·ªÉ c·ªßa b·∫°n
    option_products = products_df['product_id'].sample(n=10)
    st.session_state['option_products'] = option_products


# H√†m m√¥ ph·ªèng ƒë·ªÉ l·∫•y d·ªØ li·ªáu s·∫£n ph·∫©m
def get_sample_products(n=10):
    products = {
        'product_id': [f'P{i:04d}' for i in range(1, n+1)],
        'product_name': [f'S·∫£n ph·∫©m {i}' for i in range(1, n+1)],
        'category': np.random.choice(['Th·ªùi trang', 'ƒêi·ªán t·ª≠', 'Gia d·ª•ng', 'M·ªπ ph·∫©m', 'ƒê·ªì ch∆°i'], n),
        'price': np.random.randint(100000, 2000000, n),
        'rating': np.random.uniform(3.5, 5.0, n).round(1),
        'image_url': [f'https://picsum.photos/id/{i+20}/200/200' for i in range(n)]
    }
    return pd.DataFrame(products)

# H√†m m√¥ ph·ªèng ƒë·ªÉ l·∫•y d·ªØ li·ªáu kh√°ch h√†ng
def get_sample_customers(n=5):
    customers = {
        'customer_id': [f'C{i:04d}' for i in range(1, n+1)],
        'name': [f'Kh√°ch h√†ng {i}' for i in range(1, n+1)]
    }
    return pd.DataFrame(customers)

# H√†m m√¥ ph·ªèng k·∫øt qu·∫£ ƒë√£ ƒë·∫°t ƒë∆∞·ª£c khi hu·∫•n luy·ªán m√¥ h√¨nh
def get_content_based_results():
    results = {
        'model': ['Gensim', 'Cosine_similarity'],
        'ƒê·ªô ch√≠nh x√°c': ['Cao', 'Cao'],
        'Th·ªùi gian d·ª± ƒëo√°n': ['Kh√° nhanh (0.1722s)', 'R·∫•t nhanh (0.0035s)'],
        'L∆∞·ª£ng b·ªô nh·ªõ s·ª≠ d·ª•ng': ['Khi hu·∫•n luy·ªán t·ªën √≠t RAM h∆°n','T·ªën nhi·ªÅu RAM h∆°n'],
        'Similar_score': ['0.38-0.54', '0.36-0.56'],
    }
    return pd.DataFrame(results)


# H√†m m√¥ ph·ªèng ƒë·ªÉ t√¨m s·∫£n ph·∫©m t∆∞∆°ng t·ª± d·ª±a tr√™n n·ªôi dung
def find_similar_products(description, products_df, n=10):
    # Trong th·ª±c t·∫ø, b·∫°n s·∫Ω s·ª≠ d·ª•ng c√°c vector ƒë·∫∑c tr∆∞ng t·ª´ m√¥ h√¨nh ƒë√£ hu·∫•n luy·ªán
    # ƒê√¢y ch·ªâ l√† m√¥ ph·ªèng
    np.random.seed(hash(description) % 10000)
    similarity_scores = np.random.uniform(0.5, 1.0, len(products_df))
    products_df['similarity'] = similarity_scores
    return products_df.sort_values('similarity', ascending=False).head(n)

def is_valid_vietnamese(word):
    vietnamese_chars = (
        "a-zA-Z0-9_"
        "√†√°·∫°·∫£√£√¢·∫ß·∫•·∫≠·∫©·∫´ƒÉ·∫±·∫Ø·∫∑·∫≥·∫µ"
        "√®√©·∫π·∫ª·∫Ω√™·ªÅ·∫ø·ªá·ªÉ·ªÖ"
        "√¨√≠·ªã·ªâƒ©"
        "√≤√≥·ªç·ªè√µ√¥·ªì·ªë·ªô·ªï·ªó∆°·ªù·ªõ·ª£·ªü·ª°"
        "√π√∫·ª•·ªß≈©∆∞·ª´·ª©·ª±·ª≠·ªØ"
        "·ª≥√Ω·ªµ·ª∑·ªπ"
        "ƒë"
        "√Ä√Å·∫†·∫¢√É√Ç·∫¶·∫§·∫¨·∫®·∫™ƒÇ·∫∞·∫Æ·∫∂·∫≤·∫¥"
        "√à√â·∫∏·∫∫·∫º√ä·ªÄ·∫æ·ªÜ·ªÇ·ªÑ"
        "√å√ç·ªä·ªàƒ®"
        "√í√ì·ªå·ªé√ï√î·ªí·ªê·ªò·ªî·ªñ∆†·ªú·ªö·ª¢·ªû·ª†"
        "√ô√ö·ª§·ª¶≈®∆Ø·ª™·ª®·ª∞·ª¨·ªÆ"
        "·ª≤√ù·ª¥·ª∂·ª∏"
        "ƒê"
    )
    pattern = f'^[{vietnamese_chars}]+$'
    return re.match(pattern, word) is not None

def get_similar_document_ids(input_string, dictionary, tfidf, index, stop_words, top_k=10):
    """
    X·ª≠ l√Ω m·ªôt chu·ªói ƒë·∫ßu v√†o, t√¨m c√°c document t∆∞∆°ng t·ª± v√† tr·∫£ v·ªÅ list c√°c ID.

    Args:
        input_string (str): Chu·ªói ƒë·∫ßu v√†o c·∫ßn t√¨m ki·∫øm s·ª± t∆∞∆°ng t·ª±.
        dictionary (corpora.Dictionary): Dictionary ƒë√£ ƒë∆∞·ª£c t·∫°o t·ª´ d·ªØ li·ªáu hu·∫•n luy·ªán.
        tfidf (models.TfidfModel): M√¥ h√¨nh TF-IDF ƒë√£ ƒë∆∞·ª£c hu·∫•n luy·ªán.
        index (similarities.SparseMatrixSimilarity): Index t∆∞∆°ng t·ª± ƒë√£ ƒë∆∞·ª£c x√¢y d·ª±ng.
        stop_words (set): Set ch·ª©a c√°c stop words ti·∫øng Vi·ªát.
        top_k (int): S·ªë l∆∞·ª£ng document t∆∞∆°ng t·ª± h√†ng ƒë·∫ßu mu·ªën tr·∫£ v·ªÅ.

    Returns:
        list: Danh s√°ch c√°c ID c·ªßa c√°c document t∆∞∆°ng t·ª± nh·∫•t.
    """
    if not isinstance(input_string, str):
        return []

    # 1. Ki·ªÉm tra ti·∫øng Vi·ªát h·ª£p l·ªá v√† lo·∫°i b·ªè c√°c t·ª´ kh√¥ng h·ª£p l·ªá (t√πy ch·ªçn)
    words = input_string.split()
    valid_words = [w for w in words if is_valid_vietnamese(w)]
    processed_text = ' '.join(valid_words)
    print('dict', len(dictionary.token2id))

    # 2. Ti·∫øn h√†nh word_tokenize
    tokenized_text = word_tokenize(processed_text, format="text")
    print('tokenized',tokenized_text)

    # 3. Lo·∫°i b·ªè stop_word
    filtered_tokens = [word.lower() for word in tokenized_text.split()]
    #print('filtered_tokens',filtered_tokens)
    # 4. Chuy·ªÉn t·ª´ list c√°c t·ª´ c√≥ nghƒ©a (ƒë√£ lo·∫°i b·ªè stop words v√† vi·∫øt th∆∞·ªùng)
    meaningful_words = [re.sub('[0-9]+', '', t) for t in filtered_tokens if t not in ['', ' ', ',', '.', '...', '-', ':', ';', '?', '%', '(', ')', '+', '/', "'", '&']]
    print('meaningful_words',meaningful_words)
    # 5. S·ª≠ d·ª•ng dictionary t·∫°o tr∆∞·ªõc ƒë√≥ ƒë·ªÉ t·∫°o kw_vector
    kw_vector = dictionary.doc2bow(meaningful_words)
    print('kw_vector',kw_vector)
    # 6. sim => ƒë∆∞a v√†o m√¥ h√¨nh gensim ƒë√£ t·∫°o ·ªü tr√™n ƒë·ªÉ d·ª± ƒëo√°n k·∫øt qu·∫£
    sim = index[tfidf[kw_vector]]
    print('sim',sim)
    sorted_similarities = sorted(enumerate(sim), key=lambda item: item[1], reverse=True)
    n = 0
    top_familier = 10
    familier_lst = []
    # In k·∫øt qu·∫£ ƒë√£ s·∫Øp x·∫øp
    for doc_id, similarity in sorted_similarities:
        
        # B·ªè qua ch√≠nh n√≥ (t√πy ch·ªçn, ph·ª• thu·ªôc v√†o m·ª•c ƒë√≠ch s·ª≠ d·ª•ng)
        if doc_id != 1:  # C√≥ th·ªÉ ƒëi·ªÅu ch·ªânh ƒëi·ªÅu ki·ªán n√†y t√πy nhu c·∫ßu
            print("keyword is similar to doc_index %d: %.4f" % (doc_id, similarity))
            familier_lst.append(doc_id)
            n = n + 1
            if n>=top_familier:
                break
    print(familier_lst)
    return familier_lst

def display_product_cards(products, score_col=None):
    # T·∫°o 5 c·ªôt cho l∆∞·ªõi s·∫£n ph·∫©m
    cols = st.columns(5)
    
    # D√πng CSS ri√™ng bi·ªát thay v√¨ HTML n·ªôi tuy·∫øn
    st.markdown("""
    <style>
        .product-card {
            border: 1px solid #30363d;
            border-radius: 8px;
            padding: 10px;
            margin-bottom: 15px;
            height: 380px;
            position: relative;
            overflow: hidden;
            background-color: #121212;
        }
        .product-image {
            height: 180px;
            display: flex;
            align-items: center;
            justify-content: center;
            margin-bottom: 10px;
        }
        .product-image img {
            max-width: 100%;
            max-height: 100%;
            object-fit: contain;
        }
        .product-title {
            font-weight: bold;
            font-size: 14px;
            margin-bottom: 8px;
            height: 40px;
            overflow: hidden;
            text-overflow: ellipsis;
            display: -webkit-box;
            -webkit-line-clamp: 2;
            -webkit-box-orient: vertical;
        }
        .product-category, .product-price, .product-similarity {
            font-size: 13px;
            margin-bottom: 5px;
        }
        .product-rating {
            position: absolute;
            bottom: 10px;
            left: 10px;
            color: gold;
        }
    </style>
    """, unsafe_allow_html=True)
    
    # Hi·ªÉn th·ªã t·ª´ng s·∫£n ph·∫©m theo h√†ng v√† c·ªôt
    for i, (_, product) in enumerate(products.iterrows()):
        col_idx = i % 5
        with cols[col_idx]:
            # S·ª≠ d·ª•ng st.container ƒë·ªÉ c√°c ph·∫ßn t·ª≠ kh√¥ng b·ªã t√°ch r·ªùi
            with st.container():
                # T·∫°o th·∫ª s·∫£n ph·∫©m ho√†n ch·ªânh
                card_html = f"""
                <div class="product-card">
                    <div class="product-image">
                        <img src="{product['image'] if not pd.isna(product['image']) and product['image'] is not None else 'no_image.png'}" alt="{product['product_name']}">
                    </div>
                    <div class="product-title">{product['product_name']}</div>
                    <div class="product-category">Danh m·ª•c: {product['sub_category']}</div>
                    <div class="product-price">Gi√°: {product['price']:,} ƒë</div>
                """
                
                # Th√™m ƒëi·ªÉm t∆∞∆°ng ƒë·ªìng n·∫øu c√≥
                if score_col and score_col in product and not pd.isna(product[score_col]):
                    card_html += f'<div class="product-similarity">ƒê·ªô t∆∞∆°ng ƒë·ªìng: {product[score_col]:.2f}</div>'
                
                # Th√™m ƒë√°nh gi√° sao
                if 'rating' in product and not pd.isna(product['rating']):
                    stars = "‚≠ê" * int(product['rating'])
                    if product['rating'] % 1 >= 0.5:
                        stars += "‚ú®"
                    card_html += f'<div class="product-rating">{stars}</div>'
                
                # ƒê√≥ng th·∫ª s·∫£n ph·∫©m
                card_html += "</div>"
                
                # Hi·ªÉn th·ªã th·∫ª s·∫£n ph·∫©m
                st.markdown(card_html, unsafe_allow_html=True)

def get_user_recommendation(baseline_model,user_id,user_df,min_score=4,n=10):
    if user_id not in user_df['user_id'].unique():
        print(f"User ID {user_id} kh√¥ng t·ªìn t·∫°i trong d·ªØ li·ªáu!")
        return []
    
    # L·∫•y danh s√°ch c√°c product_id m√† ng∆∞·ªùi d√πng ƒë√£ ƒë√°nh gi√°
    #print('user_df',user_df.columns)
    user_ratings = user_df[user_df['user_id'] == user_id]
    rated_products = user_ratings['product_id'].unique()
    
    # L·∫•y danh s√°ch t·∫•t c·∫£ c√°c product_id
    all_products = user_df['product_id'].unique()
    
    # Danh s√°ch c√°c product_id m√† ng∆∞·ªùi d√πng ch∆∞a ƒë√°nh gi√°
    unrated_products = np.setdiff1d(all_products, rated_products)
    
    # D·ª± ƒëo√°n ƒë√°nh gi√° cho c√°c s·∫£n ph·∫©m ch∆∞a ƒë√°nh gi√°
    predictions = []
    for product_id in unrated_products:
        predicted_rating = baseline_model.predict(user_id, product_id).est
        if predicted_rating >= min_score:
            predictions.append((product_id, predicted_rating))
    
    # S·∫Øp x·∫øp c√°c d·ª± ƒëo√°n theo th·ª© t·ª± gi·∫£m d·∫ßn c·ªßa ƒë√°nh gi√°
    predictions.sort(key=lambda x: x[1], reverse=True)
    
    # Tr·∫£ v·ªÅ top-n ƒë·ªÅ xu·∫•t
    return predictions[:n]

def on_select_change():
    # L·∫•y gi√° tr·ªã hi·ªán t·∫°i c·ªßa selectbox t·ª´ session_state
    selected_value = st.session_state.product_selector
    
    # Th·ª±c hi·ªán c√°c h√†nh ƒë·ªông mong mu·ªën v·ªõi gi√° tr·ªã ƒë√£ ch·ªçn
    st.session_state.last_selected = selected_value
    print(f"ƒê√£ ch·ªçn s·∫£n ph·∫©m: {selected_value}")
    idx = products_df[products_df['product_id'] == int(selected_value)].index.tolist()
    print(idx)
    view_content= content_gem[idx[0]]
    kw_vector = dictionary.doc2bow(view_content)
    sim = index[tfidf[kw_vector]]
    sorted_similarities = sorted(enumerate(sim), key=lambda item: item[1], reverse=True)
    n = 0
    top_familier = 10
    familier_lst = []
    # In k·∫øt qu·∫£ ƒë√£ s·∫Øp x·∫øp
    for doc_id, similarity in sorted_similarities:
        if doc_id != idx[0]:  # C√≥ th·ªÉ ƒëi·ªÅu ch·ªânh ƒëi·ªÅu ki·ªán n√†y t√πy nhu c·∫ßu
            print("keyword is similar to doc_index %d: %.4f" % (doc_id, similarity))
            familier_lst.append(doc_id)
            n = n + 1
            if n>=top_familier:
                break
    st.session_state['similar_products_from_selectbox'] = products_df.iloc[familier_lst]



# Sidebar
st.sidebar.title("H·ªá th·ªëng ƒê·ªÅ xu·∫•t S·∫£n ph·∫©m")
st.sidebar.image("shopee_pic_1.jpg", width=250)
st.sidebar.markdown("---")

# Ch·ªçn trang trong sidebar
page = st.sidebar.selectbox(
    "Ch·ªçn ch·ª©c nƒÉng:",
    ["K·∫øt qu·∫£ hu·∫•n luy·ªán", "T√¨m s·∫£n ph·∫©m t∆∞∆°ng t·ª±", "ƒê·ªÅ xu·∫•t c√° nh√¢n h√≥a"]
)

# T·∫£i d·ªØ li·ªáu m·∫´u
sample_products = get_sample_products(50)
sample_customers = get_sample_customers()
content_based_results = get_content_based_results()
results_df = pd.read_csv('cf_algorithms_results.csv')

# Trang 1: K·∫øt qu·∫£ hu·∫•n luy·ªán
if page == "K·∫øt qu·∫£ hu·∫•n luy·ªán":
    st.title("K·∫øt qu·∫£ Hu·∫•n luy·ªán M√¥ h√¨nh ƒê·ªÅ xu·∫•t")
    
    # Hi·ªÉn th·ªã th√¥ng tin t·ªïng quan
    st.subheader("Th√¥ng tin t·ªïng quan v·ªÅ D·ª± √°n")
    col1, col2, col3 = st.columns(3)
    col1.metric("T·ªïng s·ªë s·∫£n ph·∫©m", "46,000+", "")
    col2.metric("T·ªïng s·ªë ng∆∞·ªùi d√πng", "650,000+", "")
    col3.metric("T·ªïng s·ªë ƒë√°nh gi√°", "986,000+", "")
    
    st.markdown("---")
    
    # Hi·ªÉn th·ªã b·∫£ng k·∫øt qu·∫£
    st.subheader("M√¥ h√¨nh Content-based Filtering")
    # S·ª≠ d·ª•ng Styler.set_properties ƒë·ªÉ highlight ch√≠nh x√°c d√≤ng th·ª© 2 (index 1)
    content_based_styled_df = content_based_results.style.apply(
    lambda x: ['background-color: lightgreen' if i==0 else '' 
              for i in range(len(content_based_results))],axis=0)

    st.table(content_based_styled_df)
    
    # Hi·ªÉn th·ªã bi·ªÉu ƒë·ªì so s√°nh c√°c m√¥ h√¨nh
    st.subheader("M√¥ h√¨nh User-based Filtering")
    user_based_styled_df = results_df.style.apply(
    lambda x: ['background-color: lightgreen' if i==8 else '' 
              for i in range(len(results_df))],axis=0)

    st.table(user_based_styled_df)
    

# Trang 2: T√¨m s·∫£n ph·∫©m t∆∞∆°ng t·ª±
elif page == "T√¨m s·∫£n ph·∫©m t∆∞∆°ng t·ª±":
    st.title("T√¨m S·∫£n ph·∫©m T∆∞∆°ng t·ª± (Content-based)")
    
    # Nh·∫≠p m√¥ t·∫£ s·∫£n ph·∫©m
    product_description = st.text_area(
        "Nh·∫≠p m√¥ t·∫£ s·∫£n ph·∫©m ho·∫∑c t·ª´ kh√≥a c·∫ßn t√¨m:",
        height=100,
        placeholder="V√≠ d·ª•: √°o ba l·ªó, c√† v·∫°t, m·∫Øt k√≠nh, v·ªõ, t·∫•t, √°o kho√°c..."
    )
    
    st.info("üëÜ Vui l√≤ng nh·∫≠p m√¥ t·∫£ s·∫£n ph·∫©m b·∫°n quan t√¢m ƒë·ªÉ nh·∫≠n ƒë·ªÅ xu·∫•t s·∫£n ph·∫©m t∆∞∆°ng t·ª±.")
    # N√∫t t√¨m ki·∫øm
    search_btn = st.button("T√¨m ki·∫øm", type="primary")
    
    
    if search_btn and product_description:

        input_str = str(product_description)
        similar_products_list = get_similar_document_ids(input_str, dictionary, tfidf, index, stop_words, top_k=10)
        similar_products = products_df.iloc[similar_products_list]
        #print(similar_products[['product_id','image']])

        # Hi·ªÉn th·ªã k·∫øt qu·∫£
        st.subheader(f"S·∫£n ph·∫©m t∆∞∆°ng t·ª± v·ªõi m√¥ t·∫£ c·ªßa b·∫°n:")
        st.info(f"T√¨m th·∫•y {len(similar_products)} s·∫£n ph·∫©m t∆∞∆°ng t·ª± d·ª±a tr√™n m√¥ t·∫£ c·ªßa b·∫°n.")
        
        # Hi·ªÉn th·ªã s·∫£n ph·∫©m d∆∞·ªõi d·∫°ng th·∫ª
        display_product_cards(similar_products, score_col='similarity')
    
    # Hi·ªÉn th·ªã h∆∞·ªõng d·∫´n n·∫øu ch∆∞a nh·∫≠p m√¥ t·∫£
    if not search_btn or not product_description:
        # Hi·ªÉn th·ªã m·ªôt s·ªë s·∫£n ph·∫©m ph·ªï bi·∫øn ƒë·ªÉ tham kh·∫£o
        st.subheader("M·ªôt s·ªë s·∫£n ph·∫©m ph·ªï bi·∫øn")
        valid_products = products_df[products_df['image'].notna()]
        popular_products = valid_products.sample(n=10)
        display_product_cards(popular_products,score_col='similarity')

        
    doc_id = st.selectbox(
            "Ch·ªçn s·∫£n ph·∫©m c·∫ßn t∆∞ v·∫•n:",
            options=st.session_state['option_products'],
            format_func=lambda x: f"{x} - {products_df[products_df['product_id'] == x]['product_name'].values[0]}",
            on_change=on_select_change,
            key="product_selector"
        )
    if 'similar_products_from_selectbox' in st.session_state:
        st.subheader("S·∫£n ph·∫©m t∆∞∆°ng t·ª± v·ªõi s·∫£n ph·∫©m b·∫°n ch·ªçn:")
        display_product_cards(st.session_state['similar_products_from_selectbox'], score_col='similarity')

# Trang 3: ƒê·ªÅ xu·∫•t c√° nh√¢n h√≥a
elif page == "ƒê·ªÅ xu·∫•t c√° nh√¢n h√≥a":
    st.title("ƒê·ªÅ xu·∫•t S·∫£n ph·∫©m C√° nh√¢n h√≥a")
    
    # T·∫°o tab ƒë·ªÉ ch·ªçn ph∆∞∆°ng ph√°p ƒë·ªÅ xu·∫•t
    #tab1 = st.tabs(["D·ª±a tr√™n ng∆∞·ªùi d√πng"])
    
    # Tab 1: ƒê·ªÅ xu·∫•t d·ª±a tr√™n ID ng∆∞·ªùi d√πng
    #with tab1:
        # Ch·ªçn ID kh√°ch h√†ng t·ª´ danh s√°ch
    customer_id = st.selectbox(
        "Ch·ªçn m√£ kh√°ch h√†ng:",
        options=user_df['user_id'].sample(n=10),
        format_func=lambda x: f"{x} - {user_df[user_df['user_id'] == x]['user'].values[0]}"
    )
    
    # Ho·∫∑c nh·∫≠p ID kh√°ch h√†ng m·ªõi
    custom_id = st.text_input("Ho·∫∑c nh·∫≠p m√£ kh√°ch h√†ng kh√°c:")
    
    if custom_id:
        customer_id = int(custom_id)
        print('customer_id',customer_id)
    
    # N√∫t t√¨m ki·∫øm
    rec_btn = st.button("L·∫•y ƒë·ªÅ xu·∫•t", key="rec_btn1", type="primary")
    
    if rec_btn and customer_id:
        # L·∫•y ƒë·ªÅ xu·∫•t s·∫£n ph·∫©m d·ª±a tr√™n ID kh√°ch h√†ng
        productid_lst = []
        if customer_id not in user_df['user_id'].unique():
            print(user_df['user_id'].unique())
            st.info(f"User ID {customer_id} kh√¥ng t·ªìn t·∫°i trong d·ªØ li·ªáu!")
        else:
            print(customer_id)
            top_recommendations = get_user_recommendation(baseline_model,customer_id,user_df)
            for i, (product_id, predicted_rating) in enumerate(top_recommendations, 1):
                productid_lst.append(product_id)
                df_recommend_list = products_df[products_df['product_id'].isin(productid_lst)]
            # Hi·ªÉn th·ªã ti√™u ƒë·ªÅ v·ªõi ID kh√°ch h√†ng
            st.subheader(f"S·∫£n ph·∫©m ƒë·ªÅ xu·∫•t cho kh√°ch h√†ng {customer_id}:")
            print(productid_lst)
            # Hi·ªÉn th·ªã s·∫£n ph·∫©m ƒë·ªÅ xu·∫•t
            display_product_cards(df_recommend_list, score_col='recommendation_score')

# Footer
st.markdown("---")
st.markdown(
    """
    <div style="text-align: center;">
        <p>H·ªá th·ªëng ƒê·ªÅ xu·∫•t S·∫£n ph·∫©m Shopee </p>
        <p>¬© 2025 - Ph√°t tri·ªÉn Cao Th·ªã Ng·ªçc Minh & Nguy·ªÖn K·∫ø Nh·ª±t</p>
    </div>
    """,
    unsafe_allow_html=True
)
